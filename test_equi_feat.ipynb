{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency Comparison Experiment (1): Equivariant Feature Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "from e3nn import o3\n",
    "from e3nn.o3 import TensorProduct, Irreps\n",
    "import time\n",
    "\n",
    "from sh2f import sh2f_channel\n",
    "from f2sh import f2sh_channel\n",
    "from fft import FFT_channel\n",
    "\n",
    "# Turning this off for torch.compile\n",
    "import e3nn\n",
    "e3nn.set_optimization_defaults(jit_script_fx=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1964082/1009682039.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  const_wigner2gaunt = torch.load(\"constants/const_wigner2gaunt.pt\")\n",
      "/tmp/ipykernel_1964082/1009682039.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sh2f_bases_dict = torch.load(\"constants/coefficient_sh2f.pt\")\n",
      "/tmp/ipykernel_1964082/1009682039.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f2sh_bases_dict = torch.load(\"constants/coefficient_f2sh.pt\")\n"
     ]
    }
   ],
   "source": [
    "const_wigner2gaunt = torch.load(\"constants/const_wigner2gaunt.pt\")\n",
    "\n",
    "sh2f_bases_dict = torch.load(\"constants/coefficient_sh2f.pt\")\n",
    "f2sh_bases_dict = torch.load(\"constants/coefficient_f2sh.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e3nn_implementation(tp, in1, in2):\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    res = tp(in1, in2, weight=torch.ones(tp.weight_numel).to(device))\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "    \n",
    "    return res, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_implementation(in1, in2, sh2f_bases, f2sh_bases):\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    in1_fourier, in2_fourier = sh2f_channel(in1, sh2f_bases), sh2f_channel(in2, sh2f_bases)\n",
    "    out_fourier = FFT_channel(in1_fourier, in2_fourier)\n",
    "    res = f2sh_channel(out_fourier, f2sh_bases)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "\n",
    "    return res.real, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GauntFullyConnectedTensorProduct(TensorProduct):\n",
    "\n",
    "    def __init__(\n",
    "        self, irreps_in1, irreps_in2, irreps_out, irrep_normalization: str = None, path_normalization: str = None, **kwargs\n",
    "    ):\n",
    "        irreps_in1 = o3.Irreps(irreps_in1)\n",
    "        irreps_in2 = o3.Irreps(irreps_in2)\n",
    "        irreps_out = o3.Irreps(irreps_out)\n",
    "\n",
    "        instr = [\n",
    "            (i_1, i_2, i_out, \"uuu\", True, const_wigner2gaunt[ir_out.l, ir_1.l, ir_2.l] ** 2)\n",
    "            for i_1, (_, ir_1) in enumerate(irreps_in1)\n",
    "            for i_2, (_, ir_2) in enumerate(irreps_in2)\n",
    "            for i_out, (_, ir_out) in enumerate(irreps_out)\n",
    "            if ir_out in ir_1 * ir_2\n",
    "        ]\n",
    "        super().__init__(\n",
    "            irreps_in1,\n",
    "            irreps_in2,\n",
    "            irreps_out,\n",
    "            instr,\n",
    "            irrep_normalization=irrep_normalization,\n",
    "            path_normalization=path_normalization,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_irreps(irreps_3D):\n",
    "    L = irreps_3D.shape[1]\n",
    "    irreps_1D = irreps_3D[:, 0, L - 1 : L].flatten()\n",
    "    for l in range(1, L):\n",
    "        irreps_1D = torch.cat((irreps_1D, (irreps_3D[:, l, -l + L - 1 : l + L].flatten())))\n",
    "    return irreps_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_input(L, num_channel):\n",
    "    '''\n",
    "    Generate random input irreps with degrees of [0, L) and channels of `num_channel`.\n",
    "    '''\n",
    "    in1_sh, in2_sh=torch.rand(num_channel, L, 2 * L - 1).to(device), torch.rand(num_channel, L, 2 * L - 1).to(device)\n",
    "    for l in range(L):\n",
    "        for m in range(-L+1,L):\n",
    "            if m<-l or m>l:\n",
    "                in1_sh[:, l, m + L - 1], in2_sh[:, l, m + L - 1] = 0, 0\n",
    "    in1_e3nn, in2_e3nn = flatten_irreps(in1_sh).to(device), flatten_irreps(in2_sh).to(device)\n",
    "    return in1_e3nn, in2_e3nn, in1_sh, in2_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_equi_feat(L, channel, n_warmup=10, n_sample=100, err_tolerance=1e-4):\n",
    "    '''\n",
    "    Compare the time and results for different implementation methods.\n",
    "    The input irreps have degrees of [0, L) and channels of `num_channel`.\n",
    "    The final results is averaged over `n_sample` experiments over random inputs.\n",
    "    The difference of the results from different method is less than `err_tolerance`.\n",
    "    '''\n",
    "\n",
    "    # e3nn needed\n",
    "    irreps_in1 = Irreps([(channel, (l, (-1)**l)) for l in range(L)])\n",
    "    irreps_in2 = Irreps([(channel, (l, (-1)**l)) for l in range(L)])\n",
    "    irreps_out = Irreps([(channel, (l, (-1)**l)) for l in range(2 * L - 1)])\n",
    "    e3nn_tp = GauntFullyConnectedTensorProduct(\n",
    "        irreps_in1, irreps_in2, irreps_out,\n",
    "        irrep_normalization='none', path_normalization='none', \n",
    "        internal_weights = False, shared_weights = True\n",
    "    ).to(device)\n",
    "\n",
    "    torch._dynamo.reset()\n",
    "    e3nn_tp = torch.compile(e3nn_tp, fullgraph=True, mode='reduce-overhead')\n",
    "\n",
    "    # gaunt needed\n",
    "    sh2f_bases, f2sh_bases = sh2f_bases_dict[L], f2sh_bases_dict[2 * L - 1]\n",
    "    sh2f_bases, f2sh_bases = sh2f_bases.to(device), f2sh_bases.to(device)\n",
    "    \n",
    "    \n",
    "    for i in range(n_warmup):\n",
    "        in1_e3nn, in2_e3nn, in1_sh, in2_sh = random_input(L, channel)\n",
    "\n",
    "        e3nn_res, e3nn_time = e3nn_implementation(e3nn_tp, in1_e3nn, in2_e3nn)\n",
    "        efficient_res, efficient_time = efficient_implementation(in1_sh, in2_sh, sh2f_bases, f2sh_bases)\n",
    "\n",
    "        efficient_res_flatten = flatten_irreps(efficient_res)\n",
    "        # TC: compare the results\n",
    "        assert (abs(e3nn_res - efficient_res_flatten) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - efficient_res_flatten).max()}!\"\n",
    "\n",
    "    # compare different methods\n",
    "    e3nn_times, efficient_times = torch.zeros(n_sample),torch.zeros(n_sample)\n",
    "    for i in range(n_sample):\n",
    "        in1_e3nn, in2_e3nn, in1_sh, in2_sh = random_input(L, channel)\n",
    "\n",
    "        e3nn_res, e3nn_time = e3nn_implementation(e3nn_tp, in1_e3nn, in2_e3nn)\n",
    "        efficient_res, efficient_time = efficient_implementation(in1_sh, in2_sh, sh2f_bases, f2sh_bases)\n",
    "\n",
    "        efficient_res_flatten = flatten_irreps(efficient_res)\n",
    "        # TC: compare the results\n",
    "        assert (abs(e3nn_res - efficient_res_flatten) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - efficient_res_flatten).max()}!\"\n",
    "        e3nn_times[i], efficient_times[i] = e3nn_time, efficient_time\n",
    "    print(\"Sanity Check Passed!\")\n",
    "    \n",
    "    # @T.C: compare the time\n",
    "    e3nn_mean, e3nn_std = e3nn_times.mean(), e3nn_times.std()\n",
    "    efficient_mean, efficient_std = efficient_times.mean(), efficient_times.std()\n",
    "    print(f\"e3nn takes {e3nn_mean*1000:.2f} ± {e3nn_std*1000:.2f} ms\")\n",
    "    print(f\"Efficient takes {efficient_mean*1000:.2f} ± {efficient_std*1000:.2f} ms\")\n",
    "    print(f\"L = {L}, C = {channel} efficient is {e3nn_mean / efficient_mean:.2f} x faster than e3nn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments across Different Degrees (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.19 ± 0.02 ms\n",
      "Efficient takes 0.39 ± 0.00 ms\n",
      "L = 2, C = 128 efficient is 0.48 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(2, channel=128, err_tolerance=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.20 ± 0.02 ms\n",
      "Efficient takes 0.40 ± 0.01 ms\n",
      "L = 3, C = 128 efficient is 0.50 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(3, channel=128, err_tolerance=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.26 ± 0.00 ms\n",
      "Efficient takes 0.40 ± 0.01 ms\n",
      "L = 4, C = 128 efficient is 0.65 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(4, channel=128, err_tolerance=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.40 ± 0.22 ms\n",
      "Efficient takes 0.41 ± 0.06 ms\n",
      "L = 5, C = 128 efficient is 0.99 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(5, channel=128, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.56 ± 0.00 ms\n",
      "Efficient takes 0.40 ± 0.00 ms\n",
      "L = 6, C = 128 efficient is 1.39 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(6, channel=128, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.90 ± 0.01 ms\n",
      "Efficient takes 0.40 ± 0.00 ms\n",
      "L = 7, C = 128 efficient is 2.23 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(7, channel=128, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1.22 ± 0.00 ms\n",
      "Efficient takes 0.42 ± 0.00 ms\n",
      "L = 8, C = 128 efficient is 2.89 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(8, channel=128, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1.85 ± 0.01 ms\n",
      "Efficient takes 0.48 ± 0.00 ms\n",
      "L = 9, C = 128 efficient is 3.90 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(9, channel=128, err_tolerance=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 2.79 ± 0.46 ms\n",
      "Efficient takes 0.60 ± 0.04 ms\n",
      "L = 10, C = 128 efficient is 4.62 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(10, channel=128, err_tolerance=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 3.71 ± 0.13 ms\n",
      "Efficient takes 0.67 ± 0.02 ms\n",
      "L = 11, C = 128 efficient is 5.52 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(11, channel=128, err_tolerance=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 4.79 ± 0.02 ms\n",
      "Efficient takes 0.66 ± 0.02 ms\n",
      "L = 12, C = 128 efficient is 7.22 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(12, channel=128, err_tolerance=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 6.17 ± 0.10 ms\n",
      "Efficient takes 0.76 ± 0.05 ms\n",
      "L = 13, C = 128 efficient is 8.16 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(13, channel=128, err_tolerance=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 7.76 ± 0.28 ms\n",
      "Efficient takes 0.85 ± 0.04 ms\n",
      "L = 14, C = 128 efficient is 9.09 x faster than e3nn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_feat(14, channel=128, err_tolerance=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
