{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency Comparison Experiment (2): Equivariant Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.o3 import TensorProduct, Irreps, wigner_3j, spherical_harmonics\n",
    "import e3nn\n",
    "\n",
    "# Turning this off for torch.compile\n",
    "e3nn.set_optimization_defaults(jit_script_fx=False)\n",
    "\n",
    "import time\n",
    "\n",
    "from sh2f import sh2f\n",
    "from f2sh import f2sh\n",
    "from fft import FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1957573/1009682039.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  const_wigner2gaunt = torch.load(\"constants/const_wigner2gaunt.pt\")\n",
      "/tmp/ipykernel_1957573/1009682039.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sh2f_bases_dict = torch.load(\"constants/coefficient_sh2f.pt\")\n",
      "/tmp/ipykernel_1957573/1009682039.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f2sh_bases_dict = torch.load(\"constants/coefficient_f2sh.pt\")\n"
     ]
    }
   ],
   "source": [
    "const_wigner2gaunt = torch.load(\"constants/const_wigner2gaunt.pt\")\n",
    "\n",
    "sh2f_bases_dict = torch.load(\"constants/coefficient_sh2f.pt\")\n",
    "f2sh_bases_dict = torch.load(\"constants/coefficient_f2sh.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e3nn_implementation(tp, in1, in2):   \n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    res = tp(in1, in2, weight=torch.ones(tp.weight_numel).to(device))\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "    \n",
    "    return res, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1957573/309724964.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd = torch.load(\"constants/Jd.pt\")\n"
     ]
    }
   ],
   "source": [
    "# @T.C.: Code from eSCN implementation\n",
    "\n",
    "# Borrowed from e3nn @ 0.4.0:\n",
    "# https://github.com/e3nn/e3nn/blob/0.4.0/e3nn/o3/_wigner.py#L10\n",
    "# _Jd is a list of tensors of shape (2l+1, 2l+1)\n",
    "_Jd = torch.load(\"constants/Jd.pt\")\n",
    "\n",
    "# Borrowed from e3nn @ 0.4.0:\n",
    "# https://github.com/e3nn/e3nn/blob/0.4.0/e3nn/o3/_wigner.py#L37\n",
    "#\n",
    "# In 0.5.0, e3nn shifted to torch.matrix_exp which is significantly slower:\n",
    "# https://github.com/e3nn/e3nn/blob/0.5.0/e3nn/o3/_wigner.py#L92\n",
    "\n",
    "def _z_rot_mat(angle: torch.Tensor, lv: int) -> torch.Tensor:\n",
    "        shape, device, dtype = angle.shape, angle.device, angle.dtype\n",
    "        M = angle.new_zeros((*shape, 2 * lv + 1, 2 * lv + 1))\n",
    "        inds = torch.arange(0, 2 * lv + 1, 1, device=device)\n",
    "        reversed_inds = torch.arange(2 * lv, -1, -1, device=device)\n",
    "        frequencies = torch.arange(lv, -lv - 1, -1, dtype=dtype, device=device)\n",
    "        M[..., inds, reversed_inds] = torch.sin(frequencies * angle[..., None])\n",
    "        M[..., inds, inds] = torch.cos(frequencies * angle[..., None])\n",
    "        return M\n",
    "\n",
    "def wigner_D(lval, alpha, beta, gamma):\n",
    "    if not lval < len(_Jd):\n",
    "        return o3.wigner_D(lval, torch.tensor([alpha]),torch.tensor([beta]), torch.tensor([gamma]))\n",
    "\n",
    "    alpha, beta, gamma = torch.broadcast_tensors(alpha, beta, gamma)\n",
    "    J = _Jd[lval].to(dtype=alpha.dtype, device=device)\n",
    "    Xa = _z_rot_mat(alpha, lval).to(device)\n",
    "    Xb = _z_rot_mat(beta, lval).to(device)\n",
    "    Xc = _z_rot_mat(gamma, lval).to(device)\n",
    "    return Xa @ J @ Xb @ J @ Xc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@torch.compile\n",
    "def escn(C, in1, L, r):\n",
    "    alpha, beta = o3.xyz_to_angles(r)\n",
    "    alpha, beta = alpha.to(device), beta.to(device)\n",
    "    wigner = torch.zeros(2*L-1, 4*L-3, 4*L-3, device=device)\n",
    "    for l in range(2*L-1):\n",
    "        block = wigner_D(l, torch.tensor([0.]), -beta, -alpha)\n",
    "        start = -l + 2*(L-1)\n",
    "        end = l + 2*L-1\n",
    "        wigner[l, start:end, start:end] = block\n",
    "    start_mid, end_mid = L-1, 3*L-2\n",
    "    in1_rot = torch.bmm(wigner[:L, start_mid:end_mid, start_mid:end_mid], in1.unsqueeze(-1))\n",
    "    res_rot_pos = (C[:, :, :, :, 0:1] * in1_rot.unsqueeze(0).unsqueeze(0)).sum(dim=1).sum(dim=1)\n",
    "    res_rot_neg = (C[:, :, :, :, 1:2] * in1_rot.flip(dims=[1]).unsqueeze(0).unsqueeze(0)).sum(dim=1).sum(dim=1)\n",
    "    res_rot = torch.zeros(2*L-1, 4*L-3, 1, device=device)\n",
    "    res_rot[:, start_mid:end_mid, :] = res_rot_pos + res_rot_neg\n",
    "    wigner_inv = wigner.transpose(1, 2)\n",
    "    res = torch.bmm(wigner_inv, res_rot).squeeze(-1)\n",
    "    return res\n",
    "\n",
    "def escn_implementation(C, in1, L, r):\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    res = escn(C, in1, L, r)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "    \n",
    "    return res, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@torch.compile # Currently getting  RuntimeError: self.stride(-1) must be 1 to view ComplexFloat as Float (different element sizes), but got 2\n",
    "def efficient(in1, in2, sh2f_bases, f2sh_bases):\n",
    "    in1_fourier, in2_fourier = sh2f(in1, sh2f_bases), sh2f(in2, sh2f_bases)\n",
    "    out_fourier = FFT(in1_fourier, in2_fourier)\n",
    "    res = f2sh(out_fourier, f2sh_bases)\n",
    "    return res\n",
    "\n",
    "def efficient_implementation(in1, in2, sh2f_bases, f2sh_bases):\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    res = efficient(in1, in2, sh2f_bases, f2sh_bases)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "\n",
    "    return res.real , end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GauntFullyConnectedTensorProduct(TensorProduct):\n",
    "\n",
    "    def __init__(\n",
    "        self, irreps_in1, irreps_in2, irreps_out, irrep_normalization: str = None, path_normalization: str = None, **kwargs\n",
    "    ):\n",
    "        irreps_in1 = Irreps(irreps_in1)\n",
    "        irreps_in2 = Irreps(irreps_in2)\n",
    "        irreps_out = Irreps(irreps_out)\n",
    "\n",
    "        instr = [\n",
    "            (i_1, i_2, i_out, \"uvw\", True, const_wigner2gaunt[ir_out.l, ir_1.l, ir_2.l] ** 2)\n",
    "            for i_1, (_, ir_1) in enumerate(irreps_in1)\n",
    "            for i_2, (_, ir_2) in enumerate(irreps_in2)\n",
    "            for i_out, (_, ir_out) in enumerate(irreps_out)\n",
    "            if ir_out in ir_1 * ir_2\n",
    "        ]\n",
    "        super().__init__(\n",
    "            irreps_in1,\n",
    "            irreps_in2,\n",
    "            irreps_out,\n",
    "            instr,\n",
    "            irrep_normalization=irrep_normalization,\n",
    "            path_normalization=path_normalization,\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_irreps(irreps_2D):\n",
    "    L = irreps_2D.shape[0]\n",
    "    irreps_1D = irreps_2D[0, L - 1 : L]\n",
    "    for l in range(1, L):\n",
    "        irreps_1D = torch.cat((irreps_1D, irreps_2D[l, -l + L - 1 : l + L]))\n",
    "    return irreps_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_input(L):\n",
    "    sh_coef_in1 =torch.rand(L, 2 * L - 1).to(device)\n",
    "    for l in range(L):\n",
    "        for m in range(-L+1,L):\n",
    "            if m<-l or m>l:\n",
    "                sh_coef_in1[l, m + L - 1] = 0\n",
    "    e3nn_in1 = flatten_irreps(sh_coef_in1).to(device)\n",
    "    return e3nn_in1, sh_coef_in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_spherical_harmonics_1D(L, r, normalize=True, normalization=\"norm\"):\n",
    "    res = spherical_harmonics(0, r, normalize, normalization)\n",
    "    for l in range(1, L):\n",
    "        res = torch.cat((res, o3.spherical_harmonics(l, r, normalize, normalization)))\n",
    "    return res\n",
    "\n",
    "def full_spherical_harmonics_2D(L, r, normalize=True, normalization=\"norm\"):\n",
    "    res = torch.zeros(L, 2*L-1)\n",
    "    for l in range(L):\n",
    "        res[l, -l+L-1:l+L] = spherical_harmonics(l, r, normalize, normalization)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_equi_conv(L, n_warmup=10, n_sample=100, err_tolerance=1e-4):\n",
    "    '''\n",
    "    Compare the time and results for different implementation methods.\n",
    "    The input irreps have degrees of [0, L).\n",
    "    The final results is averaged over `n_sample` experiments over random inputs.\n",
    "    The difference of the results from different method is less than `err_tolerance`.\n",
    "    '''\n",
    "\n",
    "    # e3nn needed\n",
    "    irreps_in1 = Irreps([(1, (l, (-1)**l)) for l in range(L)])\n",
    "    irreps_in2 = Irreps([(1, (l, (-1)**l)) for l in range(L)])\n",
    "    irreps_out = Irreps([(1, (l, (-1)**l)) for l in range(2 * L - 1)])\n",
    "    tp = GauntFullyConnectedTensorProduct(\n",
    "        irreps_in1, irreps_in2, irreps_out,\n",
    "        irrep_normalization='none', path_normalization='none', \n",
    "        internal_weights = False, shared_weights = True\n",
    "    ).to(device)\n",
    "\n",
    "    torch._dynamo.reset()\n",
    "    tp = torch.compile(tp, fullgraph=True, mode='reduce-overhead')\n",
    "\n",
    "    # eSCN needed\n",
    "    C = torch.zeros(2*L-1, L, L, 2*L-1, 2, device=device)\n",
    "    for lo in range(2*L-1):\n",
    "        for lf in range(L):\n",
    "            for li in range(L):\n",
    "                if abs(lf - li) <= lo <= lf + li:\n",
    "                    w3j = wigner_3j(lo, lf, li)\n",
    "                    if (lo + li + lf) % 2 == 0:\n",
    "                        m = min(li, lo)\n",
    "                        for mo in range(-m, m+1):\n",
    "                            C[lo, lf, li, mo+L-1, 0] = w3j[mo+lo, lf, mo+li]\n",
    "                            C[lo, lf, li, mo+L-1, 1] = 0 if mo==0 else w3j[mo+lo, lf, -mo+li]\n",
    "                        C[lo, lf, li] *= const_wigner2gaunt[lo, lf, li]  \n",
    "\n",
    "    # gaunt needed\n",
    "    sh2f_bases, f2sh_bases = sh2f_bases_dict[L], f2sh_bases_dict[2 * L - 1]\n",
    "    sh2f_bases, f2sh_bases = sh2f_bases.to(device), f2sh_bases.to(device)\n",
    "    \n",
    "    # compare different methods\n",
    "    e3nn_times, escn_times, efficient_times = torch.zeros(n_sample), torch.zeros(n_sample), torch.zeros(n_sample)\n",
    "    \n",
    "    for i in range(n_warmup):\n",
    "        e3nn_in1, sh_coef_in1 = random_input(L)\n",
    "        r = torch.rand(3).to(device)\n",
    "        r /= r.norm()\n",
    "        \n",
    "        e3nn_in2 = full_spherical_harmonics_1D(L, r, normalize=True, normalization=\"norm\").to(device)\n",
    "        efficient_in2 = full_spherical_harmonics_2D(L, r, normalize=True, normalization=\"norm\").to(device)\n",
    "        \n",
    "        e3nn_res, e3nn_time = e3nn_implementation(tp, e3nn_in1, e3nn_in2)\n",
    "        escn_res, escn_time = escn_implementation(C, sh_coef_in1, L, r)\n",
    "        efficient_res, efficient_time = efficient_implementation(sh_coef_in1, efficient_in2, sh2f_bases, f2sh_bases)\n",
    "\n",
    "        # TC: compare the results\n",
    "        escn_flatten = flatten_irreps(escn_res)\n",
    "        efficient_res_flatten = flatten_irreps(efficient_res)\n",
    "        assert (abs(e3nn_res - escn_flatten) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - escn_flatten).max()}\"\n",
    "        assert (abs(e3nn_res - efficient_res_flatten) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - efficient_res_flatten).max()}\"\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        e3nn_in1, sh_coef_in1 = random_input(L)\n",
    "        r = torch.rand(3).to(device)\n",
    "        r /= r.norm()\n",
    "        \n",
    "        e3nn_in2 = full_spherical_harmonics_1D(L, r, normalize=True, normalization=\"norm\").to(device)\n",
    "        efficient_in2 = full_spherical_harmonics_2D(L, r, normalize=True, normalization=\"norm\").to(device)\n",
    "        \n",
    "        e3nn_res, e3nn_time = e3nn_implementation(tp, e3nn_in1, e3nn_in2)\n",
    "        escn_res, escn_time = escn_implementation(C, sh_coef_in1, L, r)\n",
    "        efficient_res, efficient_time = efficient_implementation(sh_coef_in1, efficient_in2, sh2f_bases, f2sh_bases)\n",
    "\n",
    "        # TC: compare the results\n",
    "        escn_flatten = flatten_irreps(escn_res)\n",
    "        efficient_res_flatten = flatten_irreps(efficient_res)\n",
    "        assert (abs(e3nn_res - escn_flatten) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - escn_flatten).max()}\"\n",
    "        assert (abs(e3nn_res - efficient_res_flatten) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - efficient_res_flatten).max()}\"\n",
    "        e3nn_times[i], escn_times[i], efficient_times[i] = e3nn_time, escn_time, efficient_time\n",
    "    print(\"Sanity Check Passed!\")\n",
    "    \n",
    "    # @T.C: compare the time\n",
    "    e3nn_mean, e3nn_std = e3nn_times.mean(), e3nn_times.std()\n",
    "    escn_mean, escn_std = escn_times.mean(), escn_times.std()\n",
    "    efficient_mean, efficient_std = efficient_times.mean(), efficient_times.std()\n",
    "    print(f\"e3nn takes {e3nn_mean*1000:.2f} ± {e3nn_std*1000:.2f} ms\")\n",
    "    print(f\"escn takes {escn_mean*1000:.2f} ± {escn_std*1000:.2f} ms\")\n",
    "    print(f\"Efficient takes {efficient_mean*1000:.2f} ± {efficient_std*1000:.2f} ms\")\n",
    "    print(f\"L = {L} efficient is {e3nn_mean / efficient_mean:.2f} x faster than e3nn\")\n",
    "    print(f\"L = {L} efficient is {escn_mean / efficient_mean:.2f} x faster than escn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments across Different Degrees (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.20 ± 0.01 ms\n",
      "escn takes 1.70 ± 0.02 ms\n",
      "Efficient takes 0.38 ± 0.02 ms\n",
      "L = 2 efficient is 0.53 x faster than e3nn\n",
      "L = 2 efficient is 4.51 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=2, err_tolerance=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.24 ± 0.03 ms\n",
      "escn takes 2.58 ± 0.02 ms\n",
      "Efficient takes 0.38 ± 0.00 ms\n",
      "L = 3 efficient is 0.64 x faster than e3nn\n",
      "L = 3 efficient is 6.86 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=3, err_tolerance=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.34 ± 0.02 ms\n",
      "escn takes 3.51 ± 0.10 ms\n",
      "Efficient takes 0.38 ± 0.01 ms\n",
      "L = 4 efficient is 0.88 x faster than e3nn\n",
      "L = 4 efficient is 9.20 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=4, err_tolerance=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.48 ± 0.01 ms\n",
      "escn takes 4.32 ± 0.02 ms\n",
      "Efficient takes 0.38 ± 0.01 ms\n",
      "L = 5 efficient is 1.29 x faster than e3nn\n",
      "L = 5 efficient is 11.49 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=5, err_tolerance=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 0.69 ± 0.00 ms\n",
      "escn takes 5.15 ± 0.02 ms\n",
      "Efficient takes 0.37 ± 0.00 ms\n",
      "L = 6 efficient is 1.87 x faster than e3nn\n",
      "L = 6 efficient is 13.90 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=6, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1.03 ± 0.09 ms\n",
      "escn takes 7.97 ± 1.37 ms\n",
      "Efficient takes 0.39 ± 0.04 ms\n",
      "L = 7 efficient is 2.65 x faster than e3nn\n",
      "L = 7 efficient is 20.54 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=7, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1.48 ± 0.00 ms\n",
      "escn takes 12.04 ± 0.12 ms\n",
      "Efficient takes 0.37 ± 0.00 ms\n",
      "L = 8 efficient is 3.98 x faster than e3nn\n",
      "L = 8 efficient is 32.32 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=8, err_tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1.99 ± 0.08 ms\n",
      "escn takes 19.00 ± 0.22 ms\n",
      "Efficient takes 0.47 ± 0.03 ms\n",
      "L = 9 efficient is 4.28 x faster than e3nn\n",
      "L = 9 efficient is 40.80 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=9, err_tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 2.73 ± 0.06 ms\n",
      "escn takes 21.93 ± 0.92 ms\n",
      "Efficient takes 0.39 ± 0.03 ms\n",
      "L = 10 efficient is 6.92 x faster than e3nn\n",
      "L = 10 efficient is 55.60 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=10, err_tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 3.63 ± 0.04 ms\n",
      "escn takes 26.37 ± 1.56 ms\n",
      "Efficient takes 0.40 ± 0.04 ms\n",
      "L = 11 efficient is 9.13 x faster than e3nn\n",
      "L = 11 efficient is 66.38 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=11, err_tolerance=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 4.78 ± 0.18 ms\n",
      "escn takes 32.27 ± 2.19 ms\n",
      "Efficient takes 0.40 ± 0.03 ms\n",
      "L = 12 efficient is 11.94 x faster than e3nn\n",
      "L = 12 efficient is 80.57 x faster than escn\n"
     ]
    }
   ],
   "source": [
    "compare_equi_conv(L=12, err_tolerance=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
